{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73fc2d9b",
   "metadata": {},
   "source": [
    "# Parallel Programming with MPI and mpi4py\n",
    "\n",
    "**Computational Physics 2025**  \n",
    "**Lecture 7: Message Passing Interface (MPI)**  \n",
    "**Date: November 21, 2025**\n",
    "\n",
    "---\n",
    "\n",
    "## Lecture Outline (60 minutes)\n",
    "\n",
    "1. **Introduction to MPI** (10 min)\n",
    "   - What is MPI and why use it?\n",
    "   - MPI vs. other parallelization methods\n",
    "   - Installing and setting up mpi4py\n",
    "\n",
    "2. **Basic MPI Concepts** (15 min)\n",
    "   - Communicators, ranks, and size\n",
    "   - Point-to-point communication\n",
    "   - Collective communication\n",
    "\n",
    "3. **Practical Examples** (20 min)\n",
    "   - Hello World in MPI\n",
    "   - Data distribution and gathering\n",
    "   - Parallel numerical integration\n",
    "   - Parallel matrix operations\n",
    "\n",
    "4. **Advanced Topics** (10 min)\n",
    "   - Non-blocking communication\n",
    "   - Custom datatypes\n",
    "   - Performance considerations\n",
    "\n",
    "5. **Physics Applications** (5 min)\n",
    "   - Monte Carlo simulations\n",
    "   - N-body problems\n",
    "   - Partial differential equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a1f676",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to MPI\n",
    "\n",
    "### What is MPI?\n",
    "\n",
    "**Message Passing Interface (MPI)** is a standardized and portable message-passing system designed to function on parallel computing architectures.\n",
    "\n",
    "**Key Features:**\n",
    "- Industry standard for distributed-memory parallel computing\n",
    "- Designed for high-performance computing (HPC)\n",
    "- Works on clusters, supercomputers, and multi-core machines\n",
    "- Language-independent (C, C++, Fortran, Python via mpi4py)\n",
    "\n",
    "### Why Use MPI?\n",
    "\n",
    "1. **Scalability**: Can scale from laptops to supercomputers with thousands of nodes\n",
    "2. **Performance**: Minimal overhead, designed for efficiency\n",
    "3. **Flexibility**: Fine-grained control over communication patterns\n",
    "4. **Portability**: Code runs on different architectures with minimal changes\n",
    "\n",
    "### MPI vs. Other Parallel Programming Models\n",
    "\n",
    "| Feature | MPI | OpenMP | Threading |\n",
    "|---------|-----|--------|-----------|\n",
    "| Memory Model | Distributed | Shared | Shared |\n",
    "| Scalability | Excellent | Limited | Limited |\n",
    "| Programming Complexity | Higher | Lower | Medium |\n",
    "| Best For | Multi-node clusters | Single multi-core machine | Single multi-core machine |\n",
    "\n",
    "### Installing mpi4py\n",
    "\n",
    "**Prerequisites:** You need an MPI implementation installed:\n",
    "- **macOS**: `brew install openmpi`\n",
    "- **Linux**: `sudo apt-get install openmpi-bin libopenmpi-dev` (Ubuntu/Debian)\n",
    "- **Windows**: Use Microsoft MPI or WSL\n",
    "\n",
    "**Install mpi4py:**\n",
    "```bash\n",
    "pip install mpi4py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "232c9224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpi4py version: 4.1.1\n",
      "MPI is ready to use!\n"
     ]
    }
   ],
   "source": [
    "# Check if mpi4py is installed\n",
    "try:\n",
    "    import mpi4py\n",
    "    print(f\"mpi4py version: {mpi4py.__version__}\")\n",
    "    print(\"MPI is ready to use!\")\n",
    "except ImportError:\n",
    "    print(\"mpi4py is not installed. Please run: pip install mpi4py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99416d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic MPI Concepts\n",
    "\n",
    "### Fundamental MPI Terminology\n",
    "\n",
    "1. **Communicator**: A group of processes that can communicate with each other\n",
    "   - `MPI.COMM_WORLD`: Default communicator containing all processes\n",
    "\n",
    "2. **Rank**: Unique identifier for each process (0, 1, 2, ..., N-1)\n",
    "\n",
    "3. **Size**: Total number of processes in a communicator\n",
    "\n",
    "### The MPI Execution Model\n",
    "\n",
    "<pre>\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Same Program, Multiple Data (SPMD)         ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Rank 0  ‚îÇ  Rank 1  ‚îÇ  Rank 2  ‚îÇ  Rank 3    ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  ‚îÇCode‚îÇ  ‚îÇ  ‚îÇCode‚îÇ  ‚îÇ  ‚îÇCode‚îÇ  ‚îÇ  ‚îÇCode‚îÇ    ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ  Data A  ‚îÇ  Data B  ‚îÇ  Data C  ‚îÇ  Data D    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "</pre>\n",
    "**Key Principle**: All processes execute the same code, but can perform different operations based on their rank.\n",
    "\n",
    "### Communication Patterns\n",
    "\n",
    "1. **Point-to-Point Communication**\n",
    "   - `send()` / `recv()`: Send and receive data between specific processes\n",
    "   - `Send()` / `Recv()`: Capitalized versions for numpy arrays (faster)\n",
    "\n",
    "2. **Collective Communication**\n",
    "   - `bcast()`: Broadcast data from one process to all\n",
    "   - `scatter()`: Distribute different data to each process\n",
    "   - `gather()`: Collect data from all processes to one\n",
    "   - `reduce()`: Combine data from all processes using an operation (sum, max, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a07bf2",
   "metadata": {},
   "source": [
    "### Basic MPI Program Structure\n",
    "\n",
    "Every MPI program in Python follows this pattern:\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Get the communicator\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "# Get rank and size\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Your parallel code here\n",
    "# Different ranks can do different things\n",
    "if rank == 0:\n",
    "    # Master process\n",
    "    pass\n",
    "else:\n",
    "    # Worker processes\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Important Note**: MPI programs **cannot** be run directly in Jupyter notebooks. They must be executed using the `mpiexec` or `mpirun` command from the terminal.\n",
    "\n",
    "Let's create our first MPI program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bac7a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: mpi_hello_example.py\n",
      "\n",
      "To run this MPI program, use the terminal command:\n",
      "  mpiexec -n 4 python mpi_hello_example.py\n",
      "\n",
      "where -n 4 specifies 4 processes\n"
     ]
    }
   ],
   "source": [
    "# Create a simple MPI hello world program\n",
    "mpi_hello_code = \"\"\"from mpi4py import MPI\n",
    "import socket\n",
    "\n",
    "# Initialize MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "# Each process prints its information\n",
    "print(f\"Hello from rank {rank} of {size} processes on host {hostname}\")\n",
    "\n",
    "# Synchronize all processes\n",
    "comm.Barrier()\n",
    "\n",
    "# Only rank 0 prints the summary\n",
    "if rank == 0:\n",
    "    print(f\"\\\\n{'='*50}\")\n",
    "    print(f\"MPI program completed with {size} processes\")\n",
    "    print(f\"{'='*50}\")\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "with open('mpi_hello_example.py', 'w') as f:\n",
    "    f.write(mpi_hello_code)\n",
    "\n",
    "print(\"Created: mpi_hello_example.py\")\n",
    "print(\"\\nTo run this MPI program, use the terminal command:\")\n",
    "print(\"  mpiexec -n 4 python mpi_hello_example.py\")\n",
    "print(\"\\nwhere -n 4 specifies 4 processes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05998c4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Point-to-Point Communication\n",
    "\n",
    "### Send and Receive\n",
    "\n",
    "Point-to-point communication involves sending data from one process to another specific process.\n",
    "\n",
    "**Two flavors in mpi4py:**\n",
    "\n",
    "1. **Lowercase methods** (`send`, `recv`): For general Python objects\n",
    "   - More flexible but slower\n",
    "   - Uses pickle serialization\n",
    "   \n",
    "2. **Uppercase methods** (`Send`, `Recv`): For NumPy arrays\n",
    "   - Much faster (no serialization)\n",
    "   - Requires contiguous memory buffers\n",
    "\n",
    "### Example: Sending Python Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27cbb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: mpi_send_recv.py\n",
      "\n",
      "Run with: mpiexec -n 2 python mpi_send_recv.py\n"
     ]
    }
   ],
   "source": [
    "# Example: Point-to-point communication\n",
    "send_recv_code = \"\"\"from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if size < 2:\n",
    "    print(\"This example requires at least 2 processes\")\n",
    "    exit()\n",
    "\n",
    "# Example 1: Send/receive Python objects (lowercase)\n",
    "if rank == 0:\n",
    "    data = {'message': 'Hello from rank 0', 'value': 42, 'array': [1, 2, 3]}\n",
    "    comm.send(data, dest=1, tag=11)\n",
    "    print(f\"Rank 0 sent: {data}\")\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0, tag=11)\n",
    "    print(f\"Rank 1 received: {data}\")\n",
    "\n",
    "comm.Barrier()\n",
    "\n",
    "# Example 2: Send/receive NumPy arrays (uppercase - FASTER!)\n",
    "if rank == 0:\n",
    "    array = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=np.float64)\n",
    "    comm.Send(array, dest=1, tag=22)\n",
    "    print(f\"\\\\nRank 0 sent NumPy array: {array}\")\n",
    "elif rank == 1:\n",
    "    array = np.empty(5, dtype=np.float64)\n",
    "    comm.Recv(array, source=0, tag=22)\n",
    "    print(f\"Rank 1 received NumPy array: {array}\")\n",
    "\"\"\"\n",
    "\n",
    "with open('mpi_send_recv.py', 'w') as f:\n",
    "    f.write(send_recv_code)\n",
    "\n",
    "print(\"Created: mpi_send_recv.py\")\n",
    "print(\"\\nRun with: mpiexec -n 2 python mpi_send_recv.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d4433",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Collective Communication\n",
    "\n",
    "Collective operations involve **all** processes in a communicator. These are highly optimized and are the backbone of efficient parallel algorithms.\n",
    "\n",
    "### 4.1 Broadcast (`bcast`)\n",
    "\n",
    "Sends data from one process (root) to all other processes.\n",
    "\n",
    "<pre>\n",
    "Before:           After:\n",
    "Rank 0: [A]       Rank 0: [A]\n",
    "Rank 1: [ ]   ‚Üí   Rank 1: [A]\n",
    "Rank 2: [ ]       Rank 2: [A]\n",
    "Rank 3: [ ]       Rank 3: [A]\n",
    "</pre>\n",
    "\n",
    "### 4.2 Scatter (`scatter`)\n",
    "\n",
    "Distributes different portions of data to each process.\n",
    "\n",
    "<pre>\n",
    "Before:                After:\n",
    "Rank 0: [A,B,C,D]      Rank 0: [A]\n",
    "Rank 1: [ ]        ‚Üí   Rank 1: [B]\n",
    "Rank 2: [ ]            Rank 2: [C]\n",
    "Rank 3: [ ]            Rank 3: [D]\n",
    "</pre>\n",
    "\n",
    "### 4.3 Gather (`gather`)\n",
    "\n",
    "Collects data from all processes to one process.\n",
    "\n",
    "<pre>\n",
    "Before:           After:\n",
    "Rank 0: [A]       Rank 0: [A,B,C,D]\n",
    "Rank 1: [B]   ‚Üí   Rank 1: [B]\n",
    "Rank 2: [C]       Rank 2: [C]\n",
    "Rank 3: [D]       Rank 3: [D]\n",
    "</pre>\n",
    "\n",
    "### 4.4 Reduce (`reduce`)\n",
    "\n",
    "Combines data from all processes using an operation (SUM, MAX, MIN, etc.).\n",
    "\n",
    "<pre>\n",
    "Before:           After (SUM):\n",
    "Rank 0: [1]       Rank 0: [10]\n",
    "Rank 1: [2]   ‚Üí   Rank 1: [2]\n",
    "Rank 2: [3]       Rank 2: [3]\n",
    "Rank 3: [4]       Rank 3: [4]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac641f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: mpi_collective.py\n",
      "\n",
      "Run with: mpiexec -n 4 python mpi_collective.py\n"
     ]
    }
   ],
   "source": [
    "# Example: Collective communication operations\n",
    "collective_code = \"\"\"from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "print(f\"\\\\n{'='*60}\")\n",
    "print(f\"Process {rank} starting collective communication examples\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Example 1: Broadcast\n",
    "if rank == 0:\n",
    "    data = {'param': 'simulation_config', 'timesteps': 1000, 'dt': 0.01}\n",
    "    print(f\"\\\\nRank 0 broadcasting: {data}\")\n",
    "else:\n",
    "    data = None\n",
    "\n",
    "data = comm.bcast(data, root=0)\n",
    "print(f\"Rank {rank} received broadcast: {data}\")\n",
    "\n",
    "comm.Barrier()\n",
    "\n",
    "# Example 2: Scatter\n",
    "if rank == 0:\n",
    "    # Split work among processes\n",
    "    work = np.arange(size * 4).reshape(size, 4)\n",
    "    print(f\"\\\\nRank 0 scattering work:\\\\n{work}\")\n",
    "else:\n",
    "    work = None\n",
    "\n",
    "local_work = comm.scatter(work, root=0)\n",
    "print(f\"Rank {rank} received: {local_work}\")\n",
    "\n",
    "comm.Barrier()\n",
    "\n",
    "# Example 3: Gather\n",
    "# Each process computes something\n",
    "local_result = (rank + 1) ** 2\n",
    "print(f\"\\\\nRank {rank} computed: {local_result}\")\n",
    "\n",
    "all_results = comm.gather(local_result, root=0)\n",
    "if rank == 0:\n",
    "    print(f\"Rank 0 gathered all results: {all_results}\")\n",
    "\n",
    "comm.Barrier()\n",
    "\n",
    "# Example 4: Reduce (sum)\n",
    "local_value = np.array([rank + 1], dtype=np.float64)\n",
    "total = np.zeros(1, dtype=np.float64)\n",
    "\n",
    "comm.Reduce(local_value, total, op=MPI.SUM, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"\\\\nSum of all ranks (1+2+...+{size}): {total[0]}\")\n",
    "    print(f\"Expected: {size * (size + 1) // 2}\")\n",
    "\n",
    "# Example 5: Allreduce (result available on all processes)\n",
    "local_array = np.ones(3) * (rank + 1)\n",
    "global_sum = np.zeros(3)\n",
    "\n",
    "comm.Allreduce(local_array, global_sum, op=MPI.SUM)\n",
    "print(f\"Rank {rank} sees global sum: {global_sum}\")\n",
    "\"\"\"\n",
    "\n",
    "with open('mpi_collective.py', 'w') as f:\n",
    "    f.write(collective_code)\n",
    "\n",
    "print(\"Created: mpi_collective.py\")\n",
    "print(\"\\nRun with: mpiexec -n 4 python mpi_collective.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b409ad41",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Practical Application: Parallel Numerical Integration\n",
    "\n",
    "Let's compute œÄ using the Monte Carlo method and parallel trapezoidal integration.\n",
    "\n",
    "### Monte Carlo œÄ Calculation\n",
    "\n",
    "The idea: \n",
    "- Generate random points in a unit square\n",
    "- Count how many fall inside a quarter circle\n",
    "- œÄ ‚âà 4 √ó (points inside circle) / (total points)\n",
    "\n",
    "Each process generates its own random points, then we sum the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc898b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: mpi_pi_monte_carlo.py\n",
      "\n",
      "Run with different numbers of processes to see speedup:\n",
      "  mpiexec -n 1 python mpi_pi_monte_carlo.py\n",
      "  mpiexec -n 2 python mpi_pi_monte_carlo.py\n",
      "  mpiexec -n 4 python mpi_pi_monte_carlo.py\n",
      "  mpiexec -n 8 python mpi_pi_monte_carlo.py\n"
     ]
    }
   ],
   "source": [
    "# Example: Parallel Monte Carlo calculation of œÄ\n",
    "pi_monte_carlo = \"\"\"from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Total number of samples\n",
    "N_total = 100_000_000\n",
    "\n",
    "# Each process handles a portion\n",
    "N_local = N_total // size\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate random points in unit square\n",
    "np.random.seed(rank)  # Different seed for each process\n",
    "x = np.random.random(N_local)\n",
    "y = np.random.random(N_local)\n",
    "\n",
    "# Count points inside quarter circle\n",
    "inside = np.sum(x**2 + y**2 <= 1.0)\n",
    "\n",
    "# Sum results from all processes\n",
    "total_inside = comm.reduce(inside, op=MPI.SUM, root=0)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "if rank == 0:\n",
    "    pi_estimate = 4.0 * total_inside / N_total\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Parallel Monte Carlo Estimation of œÄ\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Number of processes: {size}\")\n",
    "    print(f\"Total samples: {N_total:,}\")\n",
    "    print(f\"Samples per process: {N_local:,}\")\n",
    "    print(f\"\\\\nEstimated œÄ: {pi_estimate:.10f}\")\n",
    "    print(f\"Actual œÄ:    {np.pi:.10f}\")\n",
    "    print(f\"Error:       {abs(pi_estimate - np.pi):.10f}\")\n",
    "    print(f\"\\\\nTime taken: {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Samples/sec: {N_total / (end_time - start_time):,.0f}\")\n",
    "\"\"\"\n",
    "\n",
    "with open('mpi_pi_monte_carlo.py', 'w') as f:\n",
    "    f.write(pi_monte_carlo)\n",
    "\n",
    "print(\"Created: mpi_pi_monte_carlo.py\")\n",
    "print(\"\\nRun with different numbers of processes to see speedup:\")\n",
    "print(\"  mpiexec -n 1 python mpi_pi_monte_carlo.py\")\n",
    "print(\"  mpiexec -n 2 python mpi_pi_monte_carlo.py\")\n",
    "print(\"  mpiexec -n 4 python mpi_pi_monte_carlo.py\")\n",
    "print(\"  mpiexec -n 8 python mpi_pi_monte_carlo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044be75",
   "metadata": {},
   "source": [
    "### Parallel Trapezoidal Integration\n",
    "\n",
    "Compute the integral ‚à´‚ÇÄ¬π ‚àö(1-x¬≤) dx = œÄ/4 using the trapezoidal rule in parallel.\n",
    "\n",
    "**Strategy:**\n",
    "1. Divide the integration domain among processes\n",
    "2. Each process computes its local integral\n",
    "3. Sum the results using `reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c840f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: mpi_trapezoid.py\n",
      "\n",
      "Run with: mpiexec -n 4 python mpi_trapezoid.py\n"
     ]
    }
   ],
   "source": [
    "# Example: Parallel trapezoidal integration\n",
    "trapezoid_code = \"\"\"from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def f(x):\n",
    "    \\\"\\\"\\\"Function to integrate: sqrt(1-x^2)\\\"\\\"\\\"\n",
    "    return np.sqrt(1 - x**2)\n",
    "\n",
    "def trapezoidal_rule(a, b, n, func):\n",
    "    \\\"\\\"\\\"Compute integral using trapezoidal rule\\\"\\\"\\\"\n",
    "    h = (b - a) / n\n",
    "    x = np.linspace(a, b, n + 1)\n",
    "    y = func(x)\n",
    "    integral = h * (0.5 * y[0] + np.sum(y[1:-1]) + 0.5 * y[-1])\n",
    "    return integral\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Integration parameters\n",
    "a = 0.0  # Lower bound\n",
    "b = 1.0  # Upper bound\n",
    "n_total = 10_000_000  # Total number of trapezoids\n",
    "\n",
    "# Divide work among processes\n",
    "n_local = n_total // size\n",
    "\n",
    "# Each process handles a portion of the domain\n",
    "local_a = a + rank * (b - a) / size\n",
    "local_b = a + (rank + 1) * (b - a) / size\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute local integral\n",
    "local_integral = trapezoidal_rule(local_a, local_b, n_local, f)\n",
    "\n",
    "# Sum all local integrals\n",
    "total_integral = comm.reduce(local_integral, op=MPI.SUM, root=0)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "if rank == 0:\n",
    "    # The integral equals œÄ/4, so multiply by 4\n",
    "    pi_estimate = 4.0 * total_integral\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Parallel Trapezoidal Integration\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Number of processes: {size}\")\n",
    "    print(f\"Total trapezoids: {n_total:,}\")\n",
    "    print(f\"Trapezoids per process: {n_local:,}\")\n",
    "    print(f\"\\\\nIntegral value: {total_integral:.10f}\")\n",
    "    print(f\"Estimated œÄ: {pi_estimate:.10f}\")\n",
    "    print(f\"Actual œÄ:    {np.pi:.10f}\")\n",
    "    print(f\"Error:       {abs(pi_estimate - np.pi):.10f}\")\n",
    "    print(f\"\\\\nTime taken: {end_time - start_time:.4f} seconds\")\n",
    "\"\"\"\n",
    "\n",
    "with open('mpi_trapezoid.py', 'w') as f:\n",
    "    f.write(trapezoid_code)\n",
    "\n",
    "print(\"Created: mpi_trapezoid.py\")\n",
    "print(\"\\nRun with: mpiexec -n 4 python mpi_trapezoid.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db427eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Parallel Matrix-Vector Multiplication\n",
    "\n",
    "A common operation in scientific computing: **y = A √ó x**\n",
    "\n",
    "**Parallel strategy:**\n",
    "1. Distribute rows of matrix A among processes\n",
    "2. Broadcast vector x to all processes\n",
    "3. Each process computes its portion of the result\n",
    "4. Gather results back to root process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ef37ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: mpi_matvec.py\n",
      "\n",
      "Run with: mpiexec -n 4 python mpi_matvec.py\n"
     ]
    }
   ],
   "source": [
    "# Example: Parallel matrix-vector multiplication\n",
    "matvec_code = \"\"\"from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Matrix dimensions\n",
    "N = 1000  # Size of square matrix (reduced for faster execution)\n",
    "\n",
    "# Number of rows per process\n",
    "rows_per_process = N // size\n",
    "\n",
    "# Create matrix and vector on root\n",
    "if rank == 0:\n",
    "    A = np.random.randn(N, N)\n",
    "    x = np.random.randn(N)\n",
    "    y_serial = np.zeros(N)\n",
    "    \n",
    "    # Serial computation for comparison\n",
    "    start_serial = time.time()\n",
    "    y_serial = A @ x\n",
    "    end_serial = time.time()\n",
    "    serial_time = end_serial - start_serial\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Parallel Matrix-Vector Multiplication\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Matrix size: {N} √ó {N}\")\n",
    "    print(f\"Number of processes: {size}\")\n",
    "    print(f\"Rows per process: {rows_per_process}\")\n",
    "    print(f\"\\\\nSerial time: {serial_time:.4f} seconds\")\n",
    "else:\n",
    "    A = None\n",
    "    x = None\n",
    "    y_serial = None\n",
    "    serial_time = 0.0\n",
    "\n",
    "# Start parallel timing\n",
    "comm.Barrier()\n",
    "start_parallel = time.time()\n",
    "\n",
    "# Scatter rows of A to all processes\n",
    "if rank == 0:\n",
    "    # Split A into chunks for each process\n",
    "    A_chunks = [A[i*rows_per_process:(i+1)*rows_per_process] for i in range(size)]\n",
    "else:\n",
    "    A_chunks = None\n",
    "\n",
    "local_A = comm.scatter(A_chunks, root=0)\n",
    "\n",
    "# Broadcast x to all processes\n",
    "x_local = comm.bcast(x if rank == 0 else None, root=0)\n",
    "\n",
    "# Each process computes its portion of y\n",
    "local_y = local_A @ x_local\n",
    "\n",
    "# Gather results\n",
    "y_parallel = comm.gather(local_y, root=0)\n",
    "if rank == 0:\n",
    "    y_parallel = np.concatenate(y_parallel)\n",
    "\n",
    "comm.Barrier()\n",
    "end_parallel = time.time()\n",
    "\n",
    "if rank == 0:\n",
    "    parallel_time = end_parallel - start_parallel\n",
    "    \n",
    "    print(f\"\\\\nParallel time: {parallel_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {serial_time / parallel_time:.2f}x\")\n",
    "    print(f\"Efficiency: {100 * serial_time / (parallel_time * size):.1f}%\")\n",
    "    \n",
    "    # Verify correctness\n",
    "    error = np.linalg.norm(y_serial - y_parallel) / np.linalg.norm(y_serial)\n",
    "    print(f\"\\\\nRelative error: {error:.2e}\")\n",
    "    if error < 1e-10:\n",
    "        print(\"‚úì Results match!\")\n",
    "    else:\n",
    "        print(\"‚úó Results differ!\")\n",
    "\"\"\"\n",
    "\n",
    "with open('mpi_matvec.py', 'w') as f:\n",
    "    f.write(matvec_code)\n",
    "\n",
    "print(\"Created: mpi_matvec.py\")\n",
    "print(\"\\nRun with: mpiexec -n 4 python mpi_matvec.py\")\n",
    "print(\"\\nNote: This version uses lowercase scatter/bcast/gather for automatic\")\n",
    "print(\"      data handling. Matrix size reduced to 1000 for faster execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648546bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Advanced Topics\n",
    "\n",
    "### 7.1 Non-Blocking Communication\n",
    "\n",
    "**Blocking vs Non-Blocking:**\n",
    "\n",
    "- **Blocking** (`send`, `recv`): Process waits until operation completes\n",
    "- **Non-blocking** (`isend`, `irecv`): Process continues immediately, can do other work\n",
    "\n",
    "**Benefits:**\n",
    "- Overlap communication with computation\n",
    "- Avoid deadlocks in complex communication patterns\n",
    "- Better performance in many scenarios\n",
    "\n",
    "```python\n",
    "# Non-blocking example\n",
    "request = comm.isend(data, dest=1, tag=11)\n",
    "# Do other work here...\n",
    "request.wait()  # Wait for send to complete\n",
    "```\n",
    "\n",
    "### 7.2 Common MPI Operations\n",
    "\n",
    "| Operation | Description | Example Use Case |\n",
    "|-----------|-------------|------------------|\n",
    "| `MPI.SUM` | Sum reduction | Total energy, particle count |\n",
    "| `MPI.MAX` | Maximum value | Maximum temperature, error |\n",
    "| `MPI.MIN` | Minimum value | Convergence check |\n",
    "| `MPI.PROD` | Product | Determinants |\n",
    "| `MPI.LAND` | Logical AND | Convergence flags |\n",
    "| `MPI.LOR` | Logical OR | Error detection |\n",
    "\n",
    "### 7.3 Performance Tips\n",
    "\n",
    "1. **Use uppercase methods for NumPy arrays** (`Send`, `Recv`) - much faster\n",
    "2. **Minimize communication** - computation should dominate\n",
    "3. **Use collective operations** instead of loops of point-to-point\n",
    "4. **Balance the load** - all processes should have similar work\n",
    "5. **Consider data locality** - minimize data movement\n",
    "6. **Profile your code** - identify bottlenecks\n",
    "\n",
    "### 7.4 Common Pitfalls\n",
    "\n",
    "‚ö†Ô∏è **Deadlock**: Processes waiting for each other indefinitely\n",
    "```python\n",
    "# BAD: Both wait to receive before sending\n",
    "data = comm.recv(source=other_rank)\n",
    "comm.send(my_data, dest=other_rank)\n",
    "\n",
    "# GOOD: One sends first, other receives first\n",
    "if rank == 0:\n",
    "    comm.send(my_data, dest=1)\n",
    "    data = comm.recv(source=1)\n",
    "else:\n",
    "    data = comm.recv(source=0)\n",
    "    comm.send(my_data, dest=0)\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **Load imbalance**: Some processes finish much earlier than others\n",
    "\n",
    "‚ö†Ô∏è **Communication overhead**: Too much communication, not enough computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f67100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: mpi_nonblocking.py\n",
      "\n",
      "Run with: mpiexec -n 2 python mpi_nonblocking.py\n"
     ]
    }
   ],
   "source": [
    "# Example: Non-blocking communication\n",
    "nonblocking_code = \"\"\"from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if size < 2:\n",
    "    print(\"This example requires at least 2 processes\")\n",
    "    exit()\n",
    "\n",
    "N = 1_000_000\n",
    "data_to_send = np.arange(N, dtype=np.float64) * (rank + 1)\n",
    "data_to_recv = np.empty(N, dtype=np.float64)\n",
    "\n",
    "# Non-blocking send/receive\n",
    "if rank == 0:\n",
    "    # Send to rank 1, receive from rank 1\n",
    "    req_send = comm.Isend(data_to_send, dest=1, tag=0)\n",
    "    req_recv = comm.Irecv(data_to_recv, source=1, tag=1)\n",
    "    \n",
    "    # Do some computation while communication happens\n",
    "    start = time.time()\n",
    "    local_result = np.sum(data_to_send ** 2)\n",
    "    computation_time = time.time() - start\n",
    "    \n",
    "    # Wait for communication to complete\n",
    "    req_send.wait()\n",
    "    req_recv.wait()\n",
    "    \n",
    "    print(f\"Rank 0: Communication overlapped with computation!\")\n",
    "    print(f\"Computation took: {computation_time:.6f} seconds\")\n",
    "    print(f\"Received data sum: {np.sum(data_to_recv):.2e}\")\n",
    "    \n",
    "elif rank == 1:\n",
    "    # Receive from rank 0, send to rank 0\n",
    "    req_recv = comm.Irecv(data_to_recv, source=0, tag=0)\n",
    "    req_send = comm.Isend(data_to_send, dest=0, tag=1)\n",
    "    \n",
    "    # Do some computation while communication happens\n",
    "    local_result = np.sum(data_to_send ** 2)\n",
    "    \n",
    "    # Wait for communication to complete\n",
    "    req_send.wait()\n",
    "    req_recv.wait()\n",
    "    \n",
    "    print(f\"Rank 1: Received data sum: {np.sum(data_to_recv):.2e}\")\n",
    "\"\"\"\n",
    "\n",
    "with open('mpi_nonblocking.py', 'w') as f:\n",
    "    f.write(nonblocking_code)\n",
    "\n",
    "print(\"Created: mpi_nonblocking.py\")\n",
    "print(\"\\nRun with: mpiexec -n 2 python mpi_nonblocking.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23aa071",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Physics Applications with MPI\n",
    "\n",
    "### 8.1 Monte Carlo Simulations\n",
    "\n",
    "**Perfect for MPI parallelization!**\n",
    "\n",
    "Examples:\n",
    "- **Ising Model**: Each process simulates independent configurations\n",
    "- **Particle Transport**: Each process tracks different particles\n",
    "- **Quantum Monte Carlo**: Distribute walkers among processes\n",
    "\n",
    "**Key idea**: Independent random trajectories ‚Üí embarrassingly parallel\n",
    "\n",
    "### 8.2 N-Body Simulations\n",
    "\n",
    "**Challenge**: All particles interact with each other\n",
    "\n",
    "**Strategies:**\n",
    "1. **Domain Decomposition**: Divide space into regions\n",
    "2. **Particle Decomposition**: Distribute particles among processes\n",
    "3. **Force Decomposition**: Distribute force calculations\n",
    "\n",
    "**Communication pattern**: \n",
    "- Exchange boundary information between neighboring domains\n",
    "- Use `Allgather` for small N, domain decomposition for large N\n",
    "\n",
    "### 8.3 Partial Differential Equations (PDEs)\n",
    "\n",
    "**Examples**: Heat equation, wave equation, Schr√∂dinger equation\n",
    "\n",
    "**Approach:**\n",
    "1. Discretize domain into grid\n",
    "2. Distribute grid points among processes\n",
    "3. Each process updates its local points\n",
    "4. Exchange boundary values with neighbors\n",
    "\n",
    "**Pattern**: Nearest-neighbor communication (ghost cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c2a889",
   "metadata": {},
   "source": [
    "### Example: 1D Heat Equation\n",
    "\n",
    "The heat equation: ‚àÇu/‚àÇt = Œ± ‚àÇ¬≤u/‚àÇx¬≤\n",
    "\n",
    "We'll solve this using finite differences with domain decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: mpi_heat_equation.py\n",
      "\n",
      "Run with: mpiexec -n 4 python mpi_heat_equation.py\n"
     ]
    }
   ],
   "source": [
    "# Example: Parallel 1D heat equation solver\n",
    "heat_eq_code = \"\"\"from mpi4py import MPI\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Physical parameters\n",
    "L = 1.0          # Length of domain\n",
    "alpha = 0.01     # Thermal diffusivity\n",
    "T_final = 0.5    # Final time\n",
    "\n",
    "# Numerical parameters\n",
    "N_global = 1004  # Total number of grid points (must be divisible by number of processes)\n",
    "dt = 0.00001     # Time step (reduced for CFL stability)\n",
    "N_steps = int(T_final / dt)\n",
    "\n",
    "# Domain decomposition\n",
    "N_local = N_global // size\n",
    "dx = L / (N_global - 1)\n",
    "\n",
    "# CFL condition check\n",
    "cfl = alpha * dt / dx**2\n",
    "if rank == 0:\n",
    "    print(f\"CFL number: {cfl:.4f} (should be < 0.5 for stability)\")\n",
    "    if cfl >= 0.5:\n",
    "        print(\"WARNING: Unstable parameters!\")\n",
    "\n",
    "# Local grid (with ghost cells for boundaries)\n",
    "u = np.zeros(N_local + 2)\n",
    "u_new = np.zeros(N_local + 2)\n",
    "\n",
    "# Initialize temperature distribution\n",
    "x_local = np.linspace(rank * N_local * dx, (rank + 1) * N_local * dx, N_local + 2)\n",
    "u[:] = np.sin(np.pi * x_local / L)  # Initial condition: sin wave\n",
    "\n",
    "# Determine neighbors\n",
    "left_neighbor = rank - 1 if rank > 0 else MPI.PROC_NULL\n",
    "right_neighbor = rank + 1 if rank < size - 1 else MPI.PROC_NULL\n",
    "\n",
    "# Time evolution\n",
    "for step in range(N_steps):\n",
    "    # Exchange boundary values with neighbors\n",
    "    # Send right boundary to right neighbor, receive left boundary from left neighbor\n",
    "    comm.Sendrecv(u[-2:-1], dest=right_neighbor, sendtag=0,\n",
    "                  recvbuf=u[0:1], source=left_neighbor, recvtag=0)\n",
    "    \n",
    "    # Send left boundary to left neighbor, receive right boundary from right neighbor\n",
    "    comm.Sendrecv(u[1:2], dest=left_neighbor, sendtag=1,\n",
    "                  recvbuf=u[-1:], source=right_neighbor, recvtag=1)\n",
    "    \n",
    "    # Update interior points using finite difference\n",
    "    u_new[1:-1] = u[1:-1] + cfl * (u[2:] - 2*u[1:-1] + u[:-2])\n",
    "    \n",
    "    # Boundary conditions (fixed at 0)\n",
    "    if rank == 0:\n",
    "        u_new[1] = 0.0\n",
    "    if rank == size - 1:\n",
    "        u_new[-2] = 0.0\n",
    "    \n",
    "    # Swap arrays\n",
    "    u, u_new = u_new, u\n",
    "\n",
    "# Gather results for plotting\n",
    "u_global = None\n",
    "if rank == 0:\n",
    "    u_global = np.zeros(N_global)\n",
    "\n",
    "# Remove ghost cells before gathering\n",
    "u_local = u[1:-1]\n",
    "comm.Gather(u_local, u_global, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    # Plot results\n",
    "    x = np.linspace(0, L, N_global)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, np.sin(np.pi * x / L), 'b--', label='Initial', linewidth=2)\n",
    "    plt.plot(x, u_global, 'r-', label=f't = {T_final}', linewidth=2)\n",
    "    plt.xlabel('x', fontsize=12)\n",
    "    plt.ylabel('Temperature', fontsize=12)\n",
    "    plt.title(f'1D Heat Equation (Parallel with {size} processes)', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('heat_equation_parallel.png', dpi=150)\n",
    "    print(f\"\\\\nSimulation complete! Plot saved as 'heat_equation_parallel.png'\")\n",
    "    print(f\"Steps computed: {N_steps:,}\")\n",
    "    print(f\"Grid points per process: {N_local}\")\n",
    "\"\"\"\n",
    "\n",
    "with open('mpi_heat_equation.py', 'w') as f:\n",
    "    f.write(heat_eq_code)\n",
    "\n",
    "print(\"Created: mpi_heat_equation.py\")\n",
    "print(\"\\nRun with: mpiexec -n 4 python mpi_heat_equation.py\")\n",
    "print(\"\\nNote: N_global=1004 (divisible by 4), dt reduced for CFL stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c830df6b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Performance Analysis\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "1. **Speedup**: S(p) = T‚ÇÅ / T‚Çö\n",
    "   - T‚ÇÅ: Time with 1 process\n",
    "   - T‚Çö: Time with p processes\n",
    "   - Ideal: S(p) = p (linear speedup)\n",
    "\n",
    "2. **Efficiency**: E(p) = S(p) / p = T‚ÇÅ / (p √ó T‚Çö)\n",
    "   - Ideal: E(p) = 1 (100%)\n",
    "   - Typical: E(p) = 0.7-0.9 (70-90%)\n",
    "\n",
    "3. **Scalability**: How speedup changes with more processes\n",
    "   - **Strong scaling**: Fixed problem size, vary processes\n",
    "   - **Weak scaling**: Fixed problem size per process\n",
    "\n",
    "### Amdahl's Law\n",
    "\n",
    "Not all code can be parallelized. If fraction `f` is serial:\n",
    "\n",
    "**Speedup ‚â§ 1 / (f + (1-f)/p)**\n",
    "\n",
    "Example: If 10% is serial (f=0.1):\n",
    "- With 10 processes: Max speedup ‚âà 5.3√ó\n",
    "- With 100 processes: Max speedup ‚âà 9.2√ó\n",
    "- With ‚àû processes: Max speedup = 10√ó\n",
    "\n",
    "**Implication**: Even small serial portions limit scalability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4df972cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: mpi_benchmark.py\n",
      "\n",
      "To measure scaling, run with different numbers of processes:\n",
      "  mpiexec -n 1 python mpi_benchmark.py\n",
      "  mpiexec -n 2 python mpi_benchmark.py\n",
      "  mpiexec -n 4 python mpi_benchmark.py\n",
      "  mpiexec -n 8 python mpi_benchmark.py\n"
     ]
    }
   ],
   "source": [
    "# Create a benchmark script to measure scaling\n",
    "benchmark_code = \"\"\"from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def benchmark_computation(size):\n",
    "    \\\"\\\"\\\"Perform some computation to benchmark\\\"\\\"\\\"\n",
    "    N = size\n",
    "    A = np.random.randn(N, N)\n",
    "    B = np.random.randn(N, N)\n",
    "    C = A @ B\n",
    "    return np.sum(C)\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Problem sizes to test\n",
    "problem_sizes = [500, 1000, 2000, 4000]\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"MPI Scaling Benchmark\")\n",
    "    print(f\"Number of processes: {size}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "for N in problem_sizes:\n",
    "    comm.Barrier()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Each process does some work\n",
    "    local_result = benchmark_computation(N // size)\n",
    "    \n",
    "    # Combine results\n",
    "    global_result = comm.reduce(local_result, op=MPI.SUM, root=0)\n",
    "    \n",
    "    comm.Barrier()\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\\\nProblem size: {N}√ó{N}\")\n",
    "        print(f\"  Time: {elapsed:.4f} seconds\")\n",
    "        print(f\"  Time per process: {elapsed:.4f} s\")\n",
    "        \n",
    "print(f\"\\\\nRank {rank} finished benchmark\")\n",
    "\"\"\"\n",
    "\n",
    "with open('mpi_benchmark.py', 'w') as f:\n",
    "    f.write(benchmark_code)\n",
    "\n",
    "print(\"Created: mpi_benchmark.py\")\n",
    "print(\"\\nTo measure scaling, run with different numbers of processes:\")\n",
    "print(\"  mpiexec -n 1 python mpi_benchmark.py\")\n",
    "print(\"  mpiexec -n 2 python mpi_benchmark.py\")\n",
    "print(\"  mpiexec -n 4 python mpi_benchmark.py\")\n",
    "print(\"  mpiexec -n 8 python mpi_benchmark.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e107efd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Best Practices and Common Patterns\n",
    "\n",
    "### Design Patterns for MPI\n",
    "\n",
    "1. **Manager-Worker Pattern**\n",
    "   - Rank 0 distributes work and collects results\n",
    "   - Other ranks process tasks\n",
    "   - Good for embarrassingly parallel problems\n",
    "\n",
    "2. **SPMD (Single Program Multiple Data)**\n",
    "   - All processes run the same code\n",
    "   - Behavior depends on rank\n",
    "   - Most common MPI pattern\n",
    "\n",
    "3. **Pipeline Pattern**\n",
    "   - Data flows through processes in sequence\n",
    "   - Each process performs a stage of computation\n",
    "   - Good for streaming data processing\n",
    "\n",
    "### Debugging MPI Programs\n",
    "\n",
    "**Common issues:**\n",
    "1. ‚ö†Ô∏è **Deadlocks**: Use `comm.Barrier()` to synchronize and debug\n",
    "2. ‚ö†Ô∏è **Wrong tags/ranks**: Print rank and size to verify\n",
    "3. ‚ö†Ô∏è **Buffer size mismatches**: Check array sizes match\n",
    "4. ‚ö†Ô∏è **Race conditions**: Use proper synchronization\n",
    "\n",
    "**Debugging tools:**\n",
    "- Print statements with rank information\n",
    "- Use `comm.Barrier()` to isolate problems\n",
    "- Run with small number of processes first (2-4)\n",
    "- Use `MPI.COMM_WORLD.Get_attr(MPI.TAG_UB)` to check max tag value\n",
    "\n",
    "### Code Organization Tips\n",
    "\n",
    "```python\n",
    "# Good practice: Separate MPI logic from computation\n",
    "def compute_physics(data):\n",
    "    \\\"\\\"\\\"Pure computation - no MPI\\\"\\\"\\\"\n",
    "    return result\n",
    "\n",
    "def parallel_workflow():\n",
    "    \\\"\\\"\\\"MPI communication logic\\\"\\\"\\\"\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    \n",
    "    # Distribute data\n",
    "    local_data = distribute_data(comm, rank)\n",
    "    \n",
    "    # Compute (MPI-free)\n",
    "    local_result = compute_physics(local_data)\n",
    "    \n",
    "    # Gather results\n",
    "    return gather_results(comm, rank, local_result)\n",
    "```\n",
    "\n",
    "### When to Use MPI\n",
    "\n",
    "‚úÖ **Good for:**\n",
    "- Large-scale simulations on clusters\n",
    "- Problems requiring distributed memory\n",
    "- Long-running computations\n",
    "- Production scientific codes\n",
    "\n",
    "‚ùå **Avoid when:**\n",
    "- Problem fits in single-node memory\n",
    "- Development/prototyping phase\n",
    "- Interactive analysis\n",
    "- Overhead > computation time\n",
    "\n",
    "**Alternatives:**\n",
    "- **Small problems**: NumPy/SciPy (optimized serial code)\n",
    "- **Shared memory**: OpenMP, threading\n",
    "- **GPU acceleration**: CuPy, PyTorch, JAX\n",
    "- **Task parallelism**: Dask, Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1bd01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. ‚úÖ **MPI Basics**\n",
    "   - Communicators, ranks, and process organization\n",
    "   - Point-to-point communication (send/recv)\n",
    "   - Collective operations (bcast, scatter, gather, reduce)\n",
    "\n",
    "2. ‚úÖ **Practical Skills**\n",
    "   - Writing and running MPI programs with mpi4py\n",
    "   - Parallel numerical algorithms (integration, matrix operations)\n",
    "   - Solving PDEs with domain decomposition\n",
    "\n",
    "3. ‚úÖ **Advanced Concepts**\n",
    "   - Non-blocking communication\n",
    "   - Performance analysis and scaling\n",
    "   - Common pitfalls and debugging\n",
    "\n",
    "4. ‚úÖ **Physics Applications**\n",
    "   - Monte Carlo simulations\n",
    "   - Heat equation solver\n",
    "   - Matrix-vector multiplication\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "üí° **MPI is powerful but requires careful design**\n",
    "- Think about data distribution and communication patterns\n",
    "- Minimize communication overhead\n",
    "- Balance computational load\n",
    "\n",
    "üí° **Start simple, then optimize**\n",
    "- Get it working correctly first\n",
    "- Measure performance before optimizing\n",
    "- Profile to find bottlenecks\n",
    "\n",
    "üí° **Not every problem needs MPI**\n",
    "- Consider the problem size and available resources\n",
    "- Sometimes serial or shared-memory parallelism is better\n",
    "- Choose the right tool for the job\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Additional Resources\n",
    "\n",
    "### Documentation\n",
    "- **mpi4py**: https://mpi4py.readthedocs.io/\n",
    "- **MPI Standard**: https://www.mpi-forum.org/\n",
    "- **MPI Tutorial**: https://mpitutorial.com/\n",
    "\n",
    "### Books\n",
    "- *\"Parallel Programming with MPI\"* by Peter Pacheco\n",
    "- *\"Using MPI\"* by Gropp, Lusk, and Skjellum\n",
    "- *\"Python for High Performance Computing\"* by various authors\n",
    "\n",
    "### Online Courses\n",
    "- XSEDE/TACC MPI tutorials\n",
    "- LLNL HPC tutorials\n",
    "- Coursera: Parallel Programming courses\n",
    "\n",
    "### Tools\n",
    "- **Performance**: `mprof`, Intel VTune, Scalasca\n",
    "- **Debugging**: `gdb` with MPI support, Allinea DDT\n",
    "- **Profiling**: `mpiP`, TAU\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Exercises and Practice Problems\n",
    "\n",
    "### Exercise 1: Parallel Dot Product\n",
    "Write an MPI program to compute the dot product of two large vectors in parallel.\n",
    "- Distribute vectors using `scatter`\n",
    "- Each process computes local dot product\n",
    "- Use `reduce` with `MPI.SUM` to get final result\n",
    "\n",
    "### Exercise 2: Parallel Sorting\n",
    "Implement parallel merge sort:\n",
    "- Each process sorts its local data\n",
    "- Use point-to-point communication to merge results\n",
    "- Compare performance with serial sort\n",
    "\n",
    "### Exercise 3: 2D Heat Equation\n",
    "Extend the 1D heat equation to 2D:\n",
    "- Use 2D domain decomposition\n",
    "- Exchange boundary data with 4 neighbors\n",
    "- Visualize the temperature field\n",
    "\n",
    "### Exercise 4: Parallel Random Walk\n",
    "Simulate random walks in parallel:\n",
    "- Each process simulates independent walkers\n",
    "- Compute statistics (mean displacement, variance)\n",
    "- Use `gather` to collect all trajectories\n",
    "\n",
    "### Exercise 5: Mandelbrot Set\n",
    "Compute the Mandelbrot set in parallel:\n",
    "- Divide the complex plane among processes\n",
    "- Each process computes its portion\n",
    "- Gather results and create visualization\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Running the Examples\n",
    "\n",
    "All example files created in this notebook can be executed from the terminal:\n",
    "\n",
    "```bash\n",
    "# Basic examples\n",
    "mpiexec -n 4 python mpi_hello_example.py\n",
    "mpiexec -n 2 python mpi_send_recv.py\n",
    "mpiexec -n 4 python mpi_collective.py\n",
    "\n",
    "# Numerical applications\n",
    "mpiexec -n 4 python mpi_pi_monte_carlo.py\n",
    "mpiexec -n 4 python mpi_trapezoid.py\n",
    "mpiexec -n 4 python mpi_matvec.py\n",
    "\n",
    "# Advanced examples\n",
    "mpiexec -n 2 python mpi_nonblocking.py\n",
    "mpiexec -n 4 python mpi_heat_equation.py\n",
    "mpiexec -n 4 python mpi_benchmark.py\n",
    "```\n",
    "\n",
    "**Tips:**\n",
    "- Start with small number of processes (2-4) for testing\n",
    "- Increase to match your CPU cores for real runs\n",
    "- On clusters, use job submission scripts (SLURM, PBS)\n",
    "- Monitor resource usage with `top` or `htop`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a169d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Quick Reference: mpi4py Cheat Sheet\n",
    "\n",
    "### Initialization\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "```\n",
    "\n",
    "### Point-to-Point Communication\n",
    "| Method | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| `comm.send(obj, dest, tag)` | Send Python object | General data |\n",
    "| `comm.recv(source, tag)` | Receive Python object | General data |\n",
    "| `comm.Send(buf, dest, tag)` | Send NumPy array | Fast, large arrays |\n",
    "| `comm.Recv(buf, source, tag)` | Receive NumPy array | Fast, large arrays |\n",
    "| `comm.isend(obj, dest, tag)` | Non-blocking send | Overlap comm/comp |\n",
    "| `comm.irecv(source, tag)` | Non-blocking receive | Overlap comm/comp |\n",
    "\n",
    "### Collective Communication\n",
    "| Method | Description | Pattern |\n",
    "|--------|-------------|---------|\n",
    "| `comm.bcast(obj, root)` | Broadcast from root | 1 ‚Üí all |\n",
    "| `comm.scatter(data, root)` | Distribute data | 1 ‚Üí all (split) |\n",
    "| `comm.gather(data, root)` | Collect data | all ‚Üí 1 |\n",
    "| `comm.allgather(data)` | Gather to all | all ‚Üí all |\n",
    "| `comm.reduce(data, op, root)` | Reduce to root | all ‚Üí 1 (combine) |\n",
    "| `comm.allreduce(data, op)` | Reduce to all | all ‚Üí all (combine) |\n",
    "\n",
    "### Reduction Operations\n",
    "```python\n",
    "MPI.SUM     # Sum values\n",
    "MPI.MAX     # Maximum value\n",
    "MPI.MIN     # Minimum value\n",
    "MPI.PROD    # Product\n",
    "MPI.LAND    # Logical AND\n",
    "MPI.LOR     # Logical OR\n",
    "```\n",
    "\n",
    "### Synchronization\n",
    "```python\n",
    "comm.Barrier()  # Wait for all processes\n",
    "request.wait()  # Wait for non-blocking operation\n",
    "```\n",
    "\n",
    "### Common Patterns\n",
    "```python\n",
    "# Master-worker\n",
    "if rank == 0:\n",
    "    # Master process\n",
    "    pass\n",
    "else:\n",
    "    # Worker processes\n",
    "    pass\n",
    "\n",
    "# Domain decomposition\n",
    "local_n = n // size\n",
    "local_start = rank * local_n\n",
    "local_end = (rank + 1) * local_n\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì End of Lecture\n",
    "\n",
    "**Thank you!**\n",
    "\n",
    "Questions? Experiments? Bugs? \n",
    "\n",
    "Let's discuss and explore MPI together!\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run the example programs\n",
    "2. Try the exercises\n",
    "3. Apply MPI to your research problems\n",
    "4. Explore advanced features (custom communicators, derived datatypes)\n",
    "5. Profile and optimize your codes\n",
    "\n",
    "**Remember**: The best way to learn MPI is to use it on real problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7fced3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MPI Example Files Created\n",
      "======================================================================\n",
      "\n",
      "Found 9 MPI example files:\n",
      "\n",
      "  1. mpi_benchmark.py               (1,057 bytes)\n",
      "  2. mpi_collective.py              (1,580 bytes)\n",
      "  3. mpi_heat_equation.py           (2,840 bytes)\n",
      "  4. mpi_hello_example.py           (464 bytes)\n",
      "  5. mpi_matvec.py                  (2,065 bytes)\n",
      "  6. mpi_nonblocking.py             (1,408 bytes)\n",
      "  7. mpi_pi_monte_carlo.py          (1,188 bytes)\n",
      "  8. mpi_send_recv.py               (892 bytes)\n",
      "  9. mpi_trapezoid.py               (1,599 bytes)\n",
      "\n",
      "======================================================================\n",
      "To run any example:\n",
      "  mpiexec -n <num_processes> python <filename>\n",
      "\n",
      "Example:\n",
      "  mpiexec -n 4 python mpi_hello_example.py\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# List all MPI example files created\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MPI Example Files Created\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get all .py files in current directory\n",
    "py_files = sorted([f for f in glob.glob(\"mpi_*.py\") if os.path.isfile(f)])\n",
    "\n",
    "if py_files:\n",
    "    print(f\"\\nFound {len(py_files)} MPI example files:\\n\")\n",
    "    for i, filename in enumerate(py_files, 1):\n",
    "        size = os.path.getsize(filename)\n",
    "        print(f\"  {i}. {filename:<30} ({size:,} bytes)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"To run any example:\")\n",
    "    print(\"  mpiexec -n <num_processes> python <filename>\")\n",
    "    print(\"\\nExample:\")\n",
    "    print(\"  mpiexec -n 4 python mpi_hello_example.py\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"\\nNo MPI files found. Run the code cells above to create them.\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254df0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16. Bug Fixes and Testing Results\n",
    "\n",
    "### Issues Found and Fixed\n",
    "\n",
    "During testing with `mpirun -np 4 python`, two issues were discovered and fixed:\n",
    "\n",
    "#### 1. **mpi_matvec.py** - Communication Method Issues\n",
    "\n",
    "**Problem:**\n",
    "- Original code used uppercase MPI methods (`Scatter`, `Bcast`, `Gather`) which require explicit buffer specifications\n",
    "- The buffer arguments were not correctly specified, causing the program to hang indefinitely\n",
    "- The program worked conceptually but had implementation issues with memory buffers\n",
    "\n",
    "**Solution:**\n",
    "- Converted to lowercase methods (`scatter`, `bcast`, `gather`) which handle data automatically via Python's pickle protocol\n",
    "- These methods are more Pythonic and handle object serialization transparently\n",
    "- Reduced matrix size from 10,000 to 1,000 for faster execution during testing\n",
    "\n",
    "**Code Changes:**\n",
    "```python\n",
    "# BEFORE (problematic):\n",
    "comm.Scatter(A, local_A, root=0)\n",
    "comm.Bcast(x_local if rank != 0 else x, root=0)\n",
    "comm.Gather(local_y, y_parallel, root=0)\n",
    "\n",
    "# AFTER (fixed):\n",
    "if rank == 0:\n",
    "    A_chunks = [A[i*rows_per_process:(i+1)*rows_per_process] for i in range(size)]\n",
    "else:\n",
    "    A_chunks = None\n",
    "local_A = comm.scatter(A_chunks, root=0)\n",
    "x_local = comm.bcast(x if rank == 0 else None, root=0)\n",
    "y_parallel = comm.gather(local_y, root=0)\n",
    "if rank == 0:\n",
    "    y_parallel = np.concatenate(y_parallel)\n",
    "```\n",
    "\n",
    "**Result:** ‚úì Program runs successfully with 0.00e+00 relative error\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **mpi_heat_equation.py** - Multiple Numerical Issues\n",
    "\n",
    "**Problem 1: Array Size Mismatch**\n",
    "- `N_global = 1000` is not evenly divisible by 4 processes\n",
    "- The `Gather` operation requires exact buffer sizes\n",
    "- Error: `ValueError: number of entries 1002 is not a multiple of required number of blocks 4`\n",
    "\n",
    "**Solution:** Changed `N_global = 1004` (divisible by 4)\n",
    "\n",
    "**Problem 2: CFL Stability**\n",
    "- CFL number = Œ± √ó dt / dx¬≤ = 0.9980 (should be < 0.5 for stability)\n",
    "- This caused numerical instability (overflow, NaN values)\n",
    "- The simulation would blow up instead of converging\n",
    "\n",
    "**Solution:** Reduced `dt` from 0.0001 to 0.00001, bringing CFL to 0.1006\n",
    "\n",
    "**Problem 3: Gather Buffer Size**\n",
    "- Gather buffer was allocated as `N_global + 2` but should be `N_global`\n",
    "- Ghost cells were already removed before gathering\n",
    "\n",
    "**Solution:** Changed buffer allocation from `np.zeros(N_global + 2)` to `np.zeros(N_global)`\n",
    "\n",
    "**Result:** ‚úì Simulation completes successfully, plot saved\n",
    "\n",
    "---\n",
    "\n",
    "### Testing Summary\n",
    "\n",
    "All MPI example programs were tested with `mpirun -np 4 python <script>.py`:\n",
    "\n",
    "| Program | Status | Notes |\n",
    "|---------|--------|-------|\n",
    "| `mpi_hello_example.py` | ‚úÖ Pass | Works correctly |\n",
    "| `mpi_send_recv.py` | ‚úÖ Pass | Point-to-point communication working |\n",
    "| `mpi_collective.py` | ‚úÖ Pass | All collective operations working |\n",
    "| `mpi_nonblocking.py` | ‚úÖ Pass | Non-blocking communication working |\n",
    "| `mpi_trapezoid.py` | ‚úÖ Pass | œÄ ‚âà 3.1415926536 (exact!) |\n",
    "| `mpi_pi_monte_carlo.py` | ‚úÖ Pass | œÄ ‚âà 3.1416360800 (error: 4.3√ó10‚Åª‚Åµ) |\n",
    "| `mpi_matvec.py` | ‚úÖ **Fixed** | Switched to lowercase scatter/bcast/gather |\n",
    "| `mpi_benchmark.py` | ‚úÖ Pass | Scaling benchmark working |\n",
    "| `mpi_heat_equation.py` | ‚úÖ **Fixed** | Fixed N_global, dt, and buffer size |\n",
    "\n",
    "### Key Lessons Learned\n",
    "\n",
    "1. **Use lowercase methods for flexibility**: `scatter`, `bcast`, `gather` are easier to use than their uppercase counterparts for general Python objects and arrays\n",
    "\n",
    "2. **Check divisibility**: When distributing data among processes, ensure the problem size is evenly divisible by the number of processes, or handle remainders explicitly\n",
    "\n",
    "3. **Verify CFL condition**: For explicit time-stepping methods in PDEs, always check that the CFL condition is satisfied for numerical stability\n",
    "\n",
    "4. **Buffer size management**: Be careful with buffer sizes in collective operations - they must match exactly between send and receive sides\n",
    "\n",
    "5. **Test with small process counts first**: Running with 2-4 processes makes debugging much easier than starting with large-scale runs\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Results\n",
    "\n",
    "**mpi_matvec.py** (1000√ó1000 matrix, 4 processes):\n",
    "- Serial time: 0.0010 seconds\n",
    "- Parallel time: 0.0076 seconds\n",
    "- Speedup: 0.13√ó (slower due to communication overhead for small problem)\n",
    "- Relative error: 0.00e+00 ‚úì\n",
    "\n",
    "**mpi_pi_monte_carlo.py** (100M samples, 4 processes):\n",
    "- Time: 0.37 seconds\n",
    "- Throughput: 270M samples/sec\n",
    "- œÄ error: 4.3√ó10‚Åª‚Åµ\n",
    "\n",
    "**mpi_trapezoid.py** (10M trapezoids, 4 processes):\n",
    "- Time: 0.0084 seconds\n",
    "- œÄ error: 0.00e+00 ‚úì\n",
    "\n",
    "**mpi_heat_equation.py** (1004 grid points, 4 processes):\n",
    "- Steps: 49,999\n",
    "- Grid points per process: 251\n",
    "- Simulation completed successfully ‚úì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d2235",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 17. When to Use Uppercase vs Lowercase MPI Methods\n",
    "\n",
    "### Understanding the Two Flavors\n",
    "\n",
    "mpi4py provides two sets of communication methods:\n",
    "\n",
    "#### **Lowercase Methods** (`send`, `recv`, `scatter`, `bcast`, etc.)\n",
    "\n",
    "**Characteristics:**\n",
    "- Use Python's pickle protocol for serialization\n",
    "- Work with any Python object (lists, dicts, custom classes)\n",
    "- Automatic type handling and buffer management\n",
    "- More Pythonic and easier to use\n",
    "- Slower due to serialization overhead\n",
    "\n",
    "**Best for:**\n",
    "- ‚úÖ Small to medium-sized data transfers\n",
    "- ‚úÖ Complex Python objects (dictionaries, lists of mixed types)\n",
    "- ‚úÖ Prototyping and development\n",
    "- ‚úÖ When convenience matters more than performance\n",
    "- ‚úÖ Irregular data structures\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Easy and flexible\n",
    "if rank == 0:\n",
    "    data = {'config': [1, 2, 3], 'params': {'alpha': 0.1}}\n",
    "else:\n",
    "    data = None\n",
    "data = comm.bcast(data, root=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Uppercase Methods** (`Send`, `Recv`, `Scatter`, `Bcast`, etc.)\n",
    "\n",
    "**Characteristics:**\n",
    "- Work directly with memory buffers (NumPy arrays)\n",
    "- No serialization - direct memory copies\n",
    "- Requires explicit buffer specification\n",
    "- Much faster for large numerical arrays\n",
    "- More complex to use correctly\n",
    "\n",
    "**Best for:**\n",
    "- ‚úÖ Large NumPy arrays (> 1 MB)\n",
    "- ‚úÖ Production code where performance is critical\n",
    "- ‚úÖ Repeated communication in tight loops\n",
    "- ‚úÖ When you need maximum speed\n",
    "- ‚úÖ HPC applications with massive data\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Fast but requires careful buffer management\n",
    "if rank == 0:\n",
    "    data = np.random.randn(1000000)\n",
    "else:\n",
    "    data = np.empty(1000000, dtype=np.float64)\n",
    "comm.Bcast([data, MPI.DOUBLE], root=0)  # Explicit buffer spec\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Data Size | Lowercase (pickle) | Uppercase (buffer) | Speedup |\n",
    "|-----------|-------------------|-------------------|---------|\n",
    "| 1 KB | ~0.1 ms | ~0.05 ms | 2√ó |\n",
    "| 1 MB | ~10 ms | ~1 ms | 10√ó |\n",
    "| 100 MB | ~1000 ms | ~50 ms | 20√ó |\n",
    "| 1 GB | ~10 sec | ~0.5 sec | 20√ó |\n",
    "\n",
    "**Rule of thumb:** For arrays larger than 1 MB, uppercase methods are significantly faster.\n",
    "\n",
    "---\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Start with lowercase methods** during development\n",
    "   - Get the algorithm working first\n",
    "   - Easier to debug and understand\n",
    "   - Less prone to buffer size errors\n",
    "\n",
    "2. **Switch to uppercase for optimization**\n",
    "   - Profile your code first\n",
    "   - Only optimize communication bottlenecks\n",
    "   - Use for large arrays in production code\n",
    "\n",
    "3. **Hybrid approach**\n",
    "   - Use lowercase for control messages and metadata\n",
    "   - Use uppercase for bulk data transfers\n",
    "   - Example:\n",
    "   ```python\n",
    "   # Send small config with lowercase\n",
    "   config = comm.bcast(config_dict, root=0)\n",
    "   \n",
    "   # Send large array with uppercase\n",
    "   comm.Bcast([large_array, MPI.DOUBLE], root=0)\n",
    "   ```\n",
    "\n",
    "4. **Common pitfalls to avoid**\n",
    "   - ‚ùå Don't use uppercase methods with Python lists\n",
    "   - ‚ùå Don't forget to pre-allocate receive buffers for uppercase\n",
    "   - ‚ùå Don't mix uppercase and lowercase in same communication\n",
    "   - ‚ùå Don't use uppercase for small, infrequent messages\n",
    "\n",
    "---\n",
    "\n",
    "### Updated Best Practices\n",
    "\n",
    "Based on our bug fixes and testing:\n",
    "\n",
    "‚úÖ **DO:**\n",
    "- Use lowercase methods for general-purpose code\n",
    "- Ensure problem sizes are divisible by process count\n",
    "- Check numerical stability (CFL conditions, etc.)\n",
    "- Test with small process counts first (2-4)\n",
    "- Verify buffer sizes match for collective operations\n",
    "- Use `comm.Barrier()` for synchronization when debugging\n",
    "\n",
    "‚ùå **DON'T:**\n",
    "- Don't assume uppercase methods are always faster (overhead matters)\n",
    "- Don't use incorrect buffer specifications\n",
    "- Don't ignore numerical stability requirements\n",
    "- Don't parallelize everything (Amdahl's law!)\n",
    "- Don't forget to handle remainders in data distribution\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Correct Usage of Both Methods\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Lowercase for small metadata\n",
    "if rank == 0:\n",
    "    metadata = {'problem_size': 10000, 'timesteps': 1000, 'dt': 0.01}\n",
    "else:\n",
    "    metadata = None\n",
    "metadata = comm.bcast(metadata, root=0)\n",
    "\n",
    "# Uppercase for large arrays\n",
    "N = metadata['problem_size']\n",
    "if rank == 0:\n",
    "    large_array = np.random.randn(N, N)\n",
    "else:\n",
    "    large_array = np.empty((N, N), dtype=np.float64)\n",
    "\n",
    "# Note: For scatter/gather with uppercase, it's complex\n",
    "# Easier to use lowercase for scatter/gather:\n",
    "if rank == 0:\n",
    "    chunks = [large_array[i::size] for i in range(size)]\n",
    "else:\n",
    "    chunks = None\n",
    "local_data = comm.scatter(chunks, root=0)\n",
    "\n",
    "# Process local data...\n",
    "result = np.sum(local_data)\n",
    "\n",
    "# Gather results (lowercase is easier)\n",
    "all_results = comm.gather(result, root=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fd083",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Testing Complete - All Programs Working!\n",
    "\n",
    "### Summary of Changes\n",
    "\n",
    "All MPI example programs have been tested with `mpirun -np 4 python` and are now **fully functional**. \n",
    "\n",
    "**Two bugs were fixed:**\n",
    "\n",
    "1. **`mpi_matvec.py`**: Converted from uppercase to lowercase MPI methods for automatic buffer management\n",
    "2. **`mpi_heat_equation.py`**: Fixed grid size divisibility, CFL stability, and buffer allocation\n",
    "\n",
    "### Verification Commands\n",
    "\n",
    "You can verify all programs work correctly by running these commands in your terminal:\n",
    "\n",
    "```bash\n",
    "# Navigate to lecture directory\n",
    "cd lectures/20251121_lecture7\n",
    "\n",
    "# Test all programs (should all run successfully)\n",
    "mpirun -np 4 python mpi_hello_example.py\n",
    "mpirun -np 2 python mpi_send_recv.py\n",
    "mpirun -np 4 python mpi_collective.py\n",
    "mpirun -np 2 python mpi_nonblocking.py\n",
    "mpirun -np 4 python mpi_trapezoid.py\n",
    "mpirun -np 4 python mpi_pi_monte_carlo.py\n",
    "mpirun -np 4 python mpi_matvec.py\n",
    "mpirun -np 4 python mpi_benchmark.py\n",
    "mpirun -np 4 python mpi_heat_equation.py\n",
    "```\n",
    "\n",
    "### What You Should See\n",
    "\n",
    "‚úÖ **All programs complete without errors**  \n",
    "‚úÖ **Numerical results are accurate** (œÄ estimates, matrix operations)  \n",
    "‚úÖ **No deadlocks or hanging**  \n",
    "‚úÖ **Proper parallel speedup** (for appropriate problem sizes)  \n",
    "\n",
    "---\n",
    "\n",
    "**Happy Parallel Computing! üöÄ**\n",
    "\n",
    "Remember: Start simple, test thoroughly, then scale up!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
